{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Training script. Opens the data, scale, filter and augment them before apply gradient boosted trees. The objects for the scalers and ML are saved for subsequent use. "
      ],
      "metadata": {
        "id": "jbbhKcndWqB6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSHkEjsu4qqR"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler, normalize, MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.multioutput import RegressorChain\n",
        "#from sklearn.impute import IterativeImputer\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "def result_plot(y_predict, y_real, n_points=5000):\n",
        "    names = ['PHIF', 'SW', 'VSH']\n",
        "    RMSE, R2 = [], []\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "        RMSE.append(np.sqrt(mean_squared_error(y_real[:,i], y_predict[:,i])))\n",
        "        R2.append(r2_score(y_real[i], y_predict[i]))\n",
        "\n",
        "    # check the accuracy of predicted data and plot the result\n",
        "    print('RMSE:', '{:.5f}'.format(np.sqrt(mean_squared_error(y_real, y_predict))))\n",
        "    for i, name in enumerate(names):\n",
        "        print(f'    {name:5s} : {RMSE[i]:.5f}')\n",
        "    #     print(\"-\"*65)\n",
        "\n",
        "    print('R^2: ', r2_score(y_real, y_predict))\n",
        "    for i, name in enumerate(names):\n",
        "        print(f'    {name:5s} : {R2[i]:.5f}')\n",
        "\n",
        "    plt.subplots(nrows=3, ncols=2, figsize=(16, 16))\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "        plt.subplot(3, 2, i * 2 + 1)\n",
        "        plt.plot(y_real[:n_points, i])\n",
        "        plt.plot(y_predict[:n_points, i])\n",
        "        plt.legend(['True', 'Predicted'])\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel(name)\n",
        "        plt.title(name + ' Prediction Comparison')\n",
        "\n",
        "        plt.subplot(3, 2, i * 2 + 2)\n",
        "        plt.scatter(y_real[:, i], y_predict[:, i], alpha=0.01)\n",
        "        plt.xlabel('Real Value')\n",
        "        plt.ylabel('Predicted Value')\n",
        "        plt.title(name + ' Prediction Comparison')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Open the training data\n",
        "df1 = pd.read_csv('train.csv')\n",
        "\n",
        "# Replace the large values with NaN\n",
        "df1.replace(['-9999', -9999], np.nan, inplace=True)\n",
        "# Drop all the rows that have just NaN\n",
        "df1 = df1.dropna(how='all')\n",
        "# Drop the rows with NaN at the outputs\n",
        "df1 = df1.dropna(subset=['PHIF', 'SW', 'VSH'])\n",
        "\n",
        "# Keep the following inputs\n",
        "col_names =  ['DEN', 'GR', 'NEU', 'PEF', 'RDEP', 'ROP'] + list(df1.columns.values[-3:])\n",
        "df1.dropna(axis=0, subset=col_names, inplace=True)\n",
        "df1.drop(['BS','DTC', 'DTS'], axis=1, inplace=True)\n",
        "\n",
        "# Log10 of the resistivity data due to large variations\n",
        "df1['RMED']=np.log10(df1['RMED'])\n",
        "df1['RDEP']=np.log10(df1['RDEP'])\n",
        "\n",
        "# Remove outliers\n",
        "Is = IsolationForest(random_state=0).fit(df1)\n",
        "clf = Is.predict(df1)\n",
        "df1=df1[clf==1]\n",
        "\n",
        "# Augment the data using first and second derivative\n",
        "Par=np.array(df1[['CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF', 'RDEP', 'RMED']])\n",
        "D_Par=np.zeros(np.shape(Par))\n",
        "for i in range(0,8):\n",
        "    D_Par[:,i]=np.hstack((np.diff(Par[:,i]),0))\n",
        "\n",
        "DD_Par=np.zeros(np.shape(Par))\n",
        "for i in range(0,8):\n",
        "    DD_Par[:,i]=np.hstack((np.diff(D_Par[:,i]),0))\n",
        "\n",
        "p=np.hstack((Par , D_Par, DD_Par))\n",
        "\n",
        "\n",
        "ppp=np.hstack((df1[['WELLNUM']], Par , D_Par, DD_Par, df1[['PHIF', 'SW', 'VSH']]))\n",
        "\n",
        "df1 = pd.DataFrame(ppp, columns=['WELLNUM', 'CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF', 'RDEP', 'RMED', \\\n",
        "                                 'D_CALI', 'D_DEN', 'D_DENC', 'D_GR', 'D_NEU', 'D_PEF', 'D_RDEP', 'D_RMED', \\\n",
        "                                 'DD_CALI', 'DD_DEN', 'DD_DENC', 'DD_GR', 'DD_NEU', 'DD_PEF', 'DD_RDEP', 'DD_RMED', 'PHIF', 'SW', 'VSH' ], index=df1.index)\n",
        "\n",
        "f_d=df1\n",
        "\n",
        "# Scale the data\n",
        "w_num = f_d['WELLNUM']\n",
        "col=f_d.columns\n",
        "f_d.drop(['WELLNUM'], axis=1, inplace=True)\n",
        "col2=f_d.columns\n",
        "trans = RobustScaler()\n",
        "n=MinMaxScaler()\n",
        "\n",
        "gk=np.array(f_d)\n",
        "data_w = trans.fit_transform(gk[:,:-3])\n",
        "data_w = n.fit_transform(data_w)\n",
        "\n",
        "trans2 = RobustScaler()\n",
        "n2=MinMaxScaler()\n",
        "data_q = trans2.fit_transform(gk[:,-3:])\n",
        "data_q = n2.fit_transform(data_q)\n",
        "\n",
        "data=np.hstack((data_w,data_q))\n",
        "\n",
        "f_d = pd.DataFrame(data, columns=col2, index=f_d.index)\n",
        "f_d2 = pd.DataFrame(np.hstack((np.reshape(np.array(w_num),(-1,1)),data)), columns=col)\n",
        "\n",
        "\n",
        "\n",
        "# Use SMOTE between boreholes to balance the data between boreholes\n",
        "smt=SMOTE(random_state=10)\n",
        "d , w =smt.fit_resample(np.array(f_d),np.array(w_num))\n",
        "\n",
        "w2=np.reshape(w,(-1,1))\n",
        "da=np.hstack((w2,d))\n",
        "df11 = pd.DataFrame(da,columns=col)\n",
        "\n",
        "df11.drop(['WELLNUM'], axis=1, inplace=True)\n",
        "\n",
        "# Train the model using Random forest\n",
        "d2 = np.array(df11)\n",
        "X_train = d2[:, :-3]\n",
        "Y_train = d2[:, -3:]\n",
        "\n",
        "\n",
        "\n",
        "estimator1 = RegressorChain(GradientBoostingRegressor(n_estimators= 125, random_state=100))\n",
        "estimator1.fit(X_train,Y_train)\n",
        "\n",
        "filehandler = open('estimator1', 'wb')\n",
        "pickle.dump(estimator1, filehandler)\n",
        "filehandler = open('trans', 'wb')\n",
        "pickle.dump(trans, filehandler)\n",
        "filehandler = open('n', 'wb')\n",
        "pickle.dump(n, filehandler)\n",
        "\n",
        "\n",
        "filehandler = open('trans2', 'wb')\n",
        "pickle.dump(trans2, filehandler)\n",
        "filehandler = open('n2', 'wb')\n",
        "pickle.dump(n2, filehandler)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script that is used to predict the reservoir properties of the testing data\n"
      ],
      "metadata": {
        "id": "ELYGOdTWW_zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler, normalize, MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.multioutput import RegressorChain\n",
        "#from sklearn.impute import IterativeImputer\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pickle\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "\n",
        "import random\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "def result_plot(y_predict, y_real, n_points=5000):\n",
        "    names = ['PHIF', 'SW', 'VSH']\n",
        "    RMSE, R2 = [], []\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "        RMSE.append(np.sqrt(mean_squared_error(y_real[:,i], y_predict[:,i])))\n",
        "        R2.append(r2_score(y_real[i], y_predict[i]))\n",
        "\n",
        "    # check the accuracy of predicted data and plot the result\n",
        "    print('RMSE:', '{:.5f}'.format(np.sqrt(mean_squared_error(y_real, y_predict))))\n",
        "    for i, name in enumerate(names):\n",
        "        print(f'    {name:5s} : {RMSE[i]:.5f}')\n",
        "    #     print(\"-\"*65)\n",
        "\n",
        "    print('R^2: ', r2_score(y_real, y_predict))\n",
        "    for i, name in enumerate(names):\n",
        "        print(f'    {name:5s} : {R2[i]:.5f}')\n",
        "\n",
        "    plt.subplots(nrows=3, ncols=2, figsize=(16, 16))\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "        plt.subplot(3, 2, i * 2 + 1)\n",
        "        plt.plot(y_real[:n_points, i])\n",
        "        plt.plot(y_predict[:n_points, i])\n",
        "        plt.legend(['True', 'Predicted'])\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel(name)\n",
        "        plt.title(name + ' Prediction Comparison')\n",
        "\n",
        "        plt.subplot(3, 2, i * 2 + 2)\n",
        "        plt.scatter(y_real[:, i], y_predict[:, i], alpha=0.01)\n",
        "        plt.xlabel('Real Value')\n",
        "        plt.ylabel('Predicted Value')\n",
        "        plt.title(name + ' Prediction Comparison')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Open the training data\n",
        "df1 = pd.read_csv('test.csv')\n",
        "\n",
        "# Replace the large values with NaN\n",
        "df1.replace(['-9999', -9999], np.nan, inplace=True)\n",
        "# Drop all the rows that have just NaN\n",
        "# Drop the rows with NaN at the outputs\n",
        "df1.drop(['WELLNUM','DEPTH','BS','DTC', 'DTS', 'ROP'], axis=1, inplace=True)\n",
        "\n",
        "# Log10 of the resistivity data due to large variations\n",
        "df1['RMED']=np.log10(df1['RMED'])\n",
        "df1['RDEP']=np.log10(df1['RDEP'])\n",
        "\n",
        "\n",
        "\n",
        "dat = np.array(df1)\n",
        "\n",
        "\n",
        "\n",
        "# Impute the data for missing values\n",
        "imp_mean = IterativeImputer(n_nearest_features=None, imputation_order='ascending', random_state=10)\n",
        "imp_mean.fit(dat)\n",
        "dat2 = imp_mean.transform(dat)\n",
        "imputer = KNNImputer(n_neighbors=10, weights=\"uniform\")\n",
        "dat3 = imputer.fit_transform(dat)\n",
        "dat2 = (dat2 + dat3) / 2\n",
        "\n",
        "df1 = pd.DataFrame(dat2, columns=['CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF', 'RDEP', 'RMED'])\n",
        "\n",
        "# Augment the data using first and second derivative\n",
        "Par=np.array(df1[['CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF', 'RDEP', 'RMED']])\n",
        "D_Par=np.zeros(np.shape(Par))\n",
        "for i in range(0,8):\n",
        "    D_Par[:,i]=np.hstack((np.diff(Par[:,i]),0))\n",
        "\n",
        "DD_Par=np.zeros(np.shape(Par))\n",
        "for i in range(0,8):\n",
        "    DD_Par[:,i]=np.hstack((np.diff(D_Par[:,i]),0))\n",
        "\n",
        "p=np.hstack((Par , D_Par, DD_Par))\n",
        "\n",
        "df1 = pd.DataFrame(p, columns=['CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF', 'RDEP', 'RMED', \\\n",
        "                                 'D_CALI', 'D_DEN', 'D_DENC', 'D_GR', 'D_NEU', 'D_PEF', 'D_RDEP', 'D_RMED', \\\n",
        "                                 'DD_CALI', 'DD_DEN', 'DD_DENC', 'DD_GR', 'DD_NEU', 'DD_PEF', 'DD_RDEP', 'DD_RMED'], index=df1.index)\n",
        "\n",
        "\n",
        "filehandler = open('trans', 'rb')\n",
        "trans = pickle.load(filehandler)\n",
        "filehandler = open('n', 'rb')\n",
        "n = pickle.load(filehandler)\n",
        "\n",
        "gk=np.array(df1)\n",
        "data = trans.transform(gk)\n",
        "data = n.transform(data)\n",
        "\n",
        "filehandler = open('estimator1', 'rb')\n",
        "estimator1 = pickle.load(filehandler)\n",
        "\n",
        "filehandler = open('trans2', 'rb')\n",
        "trans2 = pickle.load(filehandler)\n",
        "filehandler = open('n2', 'rb')\n",
        "n2 = pickle.load(filehandler)\n",
        "\n",
        "y=estimator1.predict(data)\n",
        "\n",
        "\n",
        "yyd=n2.inverse_transform(y)\n",
        "test_predict2=trans2.inverse_transform(yyd)\n",
        "\n",
        "test_predict2=np.where(test_predict2 > 0.015, test_predict2, 0.015)\n",
        "ttg=test_predict2[:,1]\n",
        "ttg=np.where(ttg < 1, ttg, 1)\n",
        "test_predict2[:,1]=ttg\n",
        "\n",
        "\n",
        "\n",
        "col_names = ['PHIF', 'SW', 'VSH']\n",
        "# Replace team_name and num_submit\n",
        "team_name = 'VE4F'\n",
        "num_submit = 1\n",
        "\n",
        "# Please don't change codes below\n",
        "N_SAMPLES = 11275\n",
        "n_sub_dict = {1: 1, 2: 2, 3: 3}\n",
        "\n",
        "# Check submission number is correct\n",
        "try:\n",
        "    n_sub = n_sub_dict[num_submit]\n",
        "except KeyError:\n",
        "    print(f\"ERROR!!! Sumbmission Number must be in 1, 2 or 3\")\n",
        "\n",
        "# Check number of samples are correct\n",
        "if test_predict2.shape[0] != N_SAMPLES:\n",
        "    raise ValueError(f\"Number of samples {test_predict2.shape[0]} doesn't matches with the correct value {N_SAMPLES}\")\n",
        "\n",
        "# Write results to csv file\n",
        "output_result = pd.DataFrame(\n",
        "    {col_names[-3]: test_predict2[:, 0], col_names[-2]: test_predict2[:, 1], col_names[-1]: test_predict2[:, 2]})\n",
        "output_result.to_csv(path_or_buf=f'./{team_name}_{n_sub}.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7uXPlmuEXFuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}